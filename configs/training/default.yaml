# @package _global_

# Default training configuration

training:
  # Optimization
  epochs: 100
  batch_size: 64
  
  # Optimizer
  optimizer:
    name: "adamw"  # adamw, adam, sgd
    lr: 0.001
    weight_decay: 0.01
    betas: [0.9, 0.999]
    
    # SGD specific (if using SGD)
    momentum: 0.9
    nesterov: true
  
  # Learning rate scheduler
  scheduler:
    name: "cosine"  # cosine, onecycle, step, reduce_on_plateau
    
    # Cosine annealing
    T_max: 100
    eta_min: 1.0e-6
    
    # OneCycleLR settings
    max_lr: 0.01
    pct_start: 0.3
    
    # Step LR
    step_size: 30
    gamma: 0.1
    
    # Warmup
    warmup_epochs: 5
    warmup_lr: 1.0e-5
  
  # Loss function
  loss:
    name: "cross_entropy"  # cross_entropy, focal_loss, label_smoothing
    label_smoothing: 0.1  # 0.0 to disable
  
  # Gradient settings
  grad_clip: 1.0  # Gradient clipping (0.0 to disable)
  grad_accumulation_steps: 1  # Simulate larger batch sizes
  
  # Mixed precision training
  use_amp: true  # Automatic Mixed Precision
  
  # Regularization
  dropout: 0.3
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 15
    min_delta: 0.001
  
  # Checkpointing
  checkpoint:
    save_top_k: 3  # Keep top 3 models
    monitor: "val_acc"  # Metric to monitor
    mode: "max"  # max or min
    save_last: true
    every_n_epochs: 1
  
  # Validation
  val_interval: 1  # Validate every N epochs
  val_check_interval: 1.0  # Validate every N batches (1.0 = full epoch)
  
  # Logging
  log_interval: 10  # Log every N batches
