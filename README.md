# Advanced Image Classification Project

An intermediate-to-advanced level image classification project implementing modern deep learning techniques and MLOps best practices.

## ğŸ¯ Learning Objectives

This project covers intermediate/advanced concepts:

### Deep Learning Concepts
- **Transfer Learning** - Fine-tuning pre-trained models (ResNet, EfficientNet, Vision Transformers)
- **Mixed Precision Training** - Using FP16 for faster training and reduced memory
- **Learning Rate Scheduling** - Cosine annealing, OneCycleLR, warmup strategies
- **Advanced Augmentations** - CutMix, MixUp, RandAugment, AutoAugment
- **Attention Mechanisms** - Vision Transformers (ViT) and self-attention
- **Model Ensembling** - Combining multiple models for better performance
- **Gradient Accumulation** - Training with larger effective batch sizes
- **Label Smoothing** - Regularization technique for better generalization

### MLOps & Best Practices
- **Experiment Tracking** - Weights & Biases integration
- **Model Checkpointing** - Saving best models with versioning
- **Configuration Management** - Hydra for managing experiments
- **Data Versioning** - DVC concepts for reproducibility
- **Model Registry** - Organizing and versioning trained models
- **TensorBoard** - Monitoring training metrics
- **ONNX Export** - Model deployment optimization
- **Distributed Training** - Multi-GPU training basics

### Latest AI Trends (2026)
- **Vision Transformers (ViT)** - Attention-based architectures
- **Knowledge Distillation** - Compressing large models
- **Self-Supervised Learning** - SimCLR, DINO approaches
- **Neural Architecture Search (NAS)** - EfficientNet family
- **Explainability** - GradCAM, attention visualization

## ğŸ“ Project Structure

```
image_classification/
â”œâ”€â”€ configs/                    # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main config
â”‚   â”œâ”€â”€ model/                 # Model configs
â”‚   â”œâ”€â”€ data/                  # Dataset configs
â”‚   â””â”€â”€ experiment/            # Experiment presets
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py         # Custom dataset classes
â”‚   â”‚   â”œâ”€â”€ augmentations.py   # Advanced augmentation pipeline
â”‚   â”‚   â””â”€â”€ dataloader.py      # DataLoader setup
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ resnet.py          # ResNet variants
â”‚   â”‚   â”œâ”€â”€ efficientnet.py    # EfficientNet models
â”‚   â”‚   â”œâ”€â”€ vit.py             # Vision Transformer
â”‚   â”‚   â””â”€â”€ custom_models.py   # Custom architectures
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop with mixed precision
â”‚   â”‚   â”œâ”€â”€ losses.py          # Custom loss functions
â”‚   â”‚   â”œâ”€â”€ metrics.py         # Evaluation metrics
â”‚   â”‚   â””â”€â”€ callbacks.py       # Training callbacks
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py          # Config utilities
â”‚   â”‚   â”œâ”€â”€ visualization.py   # GradCAM, attention maps
â”‚   â”‚   â””â”€â”€ checkpoint.py      # Model checkpointing
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ predictor.py       # Inference pipeline
â”‚       â””â”€â”€ ensemble.py        # Model ensembling
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb          # Exploratory data analysis
â”‚   â”œâ”€â”€ 02_augmentations.ipynb # Visualize augmentations
â”‚   â””â”€â”€ 03_model_analysis.ipynb # Model interpretation
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               # Main training script
â”‚   â”œâ”€â”€ evaluate.py            # Evaluation script
â”‚   â””â”€â”€ export_onnx.py         # Model export
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

## ğŸš€ Quick Start

### Installation

```bash
# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### Training

```bash
# Basic training with ResNet50
python scripts/train.py model=resnet50

# Train Vision Transformer
python scripts/train.py model=vit_base experiment=vit_finetune

# Multi-GPU training
python scripts/train.py trainer.gpus=2 training.batch_size=64
```

### Experiment Tracking

The project uses Weights & Biases for experiment tracking:
- View training metrics in real-time
- Compare different model architectures
- Track hyperparameter performance
- Visualize model predictions and attention maps

## ğŸ§ª Intermediate Concepts Demonstrated

### 1. Mixed Precision Training
Uses PyTorch's AMP (Automatic Mixed Precision) for faster training:
```python
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()
```

### 2. Advanced Data Augmentation
Implements modern augmentation strategies:
- **RandAugment**: Automatically searches for augmentation policies
- **CutMix**: Mixes patches from different images
- **MixUp**: Blends images and labels
- **AutoAugment**: Learned augmentation strategies

### 3. Learning Rate Schedules
- Cosine Annealing with Warm Restarts
- OneCycleLR for super-convergence
- Linear warmup strategies

### 4. Vision Transformers
Implements attention-based architectures that represent the latest trend in computer vision.

### 5. Model Interpretability
- GradCAM for visualizing what the model focuses on
- Attention map visualization for ViT
- Feature map analysis

## ğŸ“Š Datasets Supported

- CIFAR-10/100 (built-in PyTorch)
- ImageNet (requires download)
- Custom datasets (via config)
- Kaggle competitions datasets

## ğŸ“ Learning Path

1. **Start**: Train ResNet50 with transfer learning
2. **Intermediate**: Experiment with advanced augmentations
3. **Advanced**: Implement Vision Transformer from scratch
4. **Expert**: Add knowledge distillation and ensemble methods

## ğŸ“– Key Libraries

- **PyTorch** - Deep learning framework
- **timm** - State-of-the-art computer vision models
- **albumentations** - Advanced image augmentations
- **wandb** - Experiment tracking
- **hydra** - Configuration management
- **torch.onnx** - Model export for deployment

## ğŸ”¥ Advanced Features to Explore

- [ ] Implement knowledge distillation
- [ ] Add self-supervised pre-training
- [ ] Implement neural architecture search
- [ ] Add test-time augmentation
- [ ] Implement gradual unfreezing
- [ ] Add adversarial training
- [ ] Implement semi-supervised learning

## ğŸ“ License

MIT License
